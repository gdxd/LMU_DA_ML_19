{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using Boosted Decision Trees (BDTs), a popular method in experimental particle physics. \n",
    "\n",
    "* BDTs are based on an ensemble of _weak classifiers_ (decision trees).\n",
    "* Boosting increases the weight of misclassified events. \n",
    "* The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**.\n",
    "  * more information about the data is available from the links, and in particular in the accompanying **[documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**.\n",
    "  * much of the description below is taken from this documentation\n",
    "* The general idea is that we want to extract $H\\to\\tau\\tau$ signal from background. \n",
    "  * first channel where coupling of Higgs boson to fermions can be proven (before only coupling to bosons, $\\gamma$, $W$, $Z$)\n",
    "* In particular, the selection requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. \n",
    "* The challenge is based on Monte Carlo collision events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHC and ATLAS\n",
    "* LHC collides bunches of protons every 50 nanoseconds inside ATLAS\n",
    "* In the hard-scattering process, two colliding protons interact and part of the kinetic energy of the protons is converted into new particles.\n",
    "* Most resulting particles are very unstable and decay quickly into a cascade of lighter particles.\n",
    "* ATLAS detector measures the properties of the decay products: type, energy and momentum (3-D direction)\n",
    "* The decay products are identified and reconstructed from the low-level analogue and digital signals they trigger in the detector hardware.\n",
    "* Part of the energy will be converted into and carried away by neutrinos (e.g. from the decay of tau leptons) that cannot be measured, leading to an incomplete event reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "The data contains > 800 k simulated collision events, that were used in the reference [ATLAS analysis][1]:\n",
    "* 250 k for training\n",
    "* 100 k for testing (public leaderboard)\n",
    "* 450 k for testing (private leaderboard)\n",
    "* a small withheld dataset\n",
    "\n",
    "Here, we use the full dataset:\n",
    "\n",
    "[1]: http://cds.cern.ch/record/1632191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"KaggleSet\").count()[\"EventId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset mixes background (b) and signal (s) events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").count()[\"EventId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the actual $s:b$ ratio were so large ($\\sim1/3$), we would have found the Higgs much earlier. \n",
    "To obtain the actual number of signal and background events we expect in the 2012 ATLAS dataset, we need to take into account the provided weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Label\").sum()[\"Weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, without any additional selection we expect a signal-background ratio of only 1.7 permille.\n",
    "\n",
    "Each simulated event has a weight\n",
    "* proportional to the conditional density divided by the instrumental density used by the simulator (an importance-sampling flavor),\n",
    "* and normalized for integrated luminosity (the size of the dataset; factors in cross section, beam intensity and run time of the collider)\n",
    "\n",
    "The weights are an artifact of the way the simulation works and not part of the input to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different weights correspond roughly to different background processes (due to the different cross sections)\n",
    "ax = plt.hist(df[\"Weight\"], bins = 100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three different background processes were retained in this dataset ($Z\\to\\tau\\tau$, top-quark-pair production, $W\\to\\ell\\nu$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief overview on variables, there is more information in the documentation. \n",
    "* 30 features\n",
    "  * The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from background. \n",
    "  * On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. \n",
    "    * They themselves generally do not provide much discrimination.\n",
    "    * One of the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. \n",
    "* *EventId* identifies the event but is not a \"feature.\" \n",
    "* The *Weight* is the event weight.\n",
    "  * used to obtain the proper normalization of the different signal and background samples\n",
    "  * sum of weights of all signal events should produce the signal yield expected to be observed in 2012 LHC data taking\n",
    "  * sum of weights of all background events should produce the background yield\n",
    "* *Label* indicates if it is a signal or background event. \n",
    "* Ignore the *Kaggle* variables --- they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate/visualize some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sub-set of vars for plotting\n",
    "varplot = ['DER_mass_MMC', 'DER_mass_jet_jet',\n",
    "       'DER_deltar_tau_lep', 'DER_pt_tot','PRI_jet_subleading_pt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: map labels to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct histograms\n",
    "for key in varplot:\n",
    "    # plotting settings\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    bins = np.linspace(min(df[key]), max(df[key]), 30)\n",
    "    # plot signal & backg\n",
    "    p = plt.hist([df[df['Label']==0][key],df[df['Label']==1][key]], bins=bins, stacked=True,label=['Backg','Signal'])\n",
    "    \n",
    "    # decorate\n",
    "    plt.xlabel(key)\n",
    "    plt.ylabel('Number of Events')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only first 10k entries\n",
    "_ = sns.pairplot(df.iloc[:1000], hue='Label', height=3.0, vars=varplot) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The diagonal plot use _kernel density estimation_ to smear out data points in phase space and add up the result to obtain a smooth function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays for ML models\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt'] # features to train on\n",
    "y = df['Label'] # labels\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split data and weights into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First ML trials w/ simple models\n",
    "1st attempt with simple models: GaussianNB and Logistic Regression\n",
    "* train\n",
    "* test\n",
    "* plot scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB (Gaussian Naive Bayes, assumes each class is drawn from an axis-aligned Gaussian distribution)\n",
    "from sklearn.naive_bayes import GaussianNB # 1. choose model class\n",
    "model = GaussianNB()                       # 2. instantiate model\n",
    "model.fit(X_train, y_train)                # 3. fit model to data\n",
    "gnb = model.predict(X_test)                # 4. predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot probability for sig/bg\n",
    "def plot_proba( df, model, x ):\n",
    "    df['Prob']=model.predict_proba(x)[:, 1]\n",
    "    kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "    df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "    df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "    _=plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_proba( df, model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression\n",
    "As next attempt, let's look at [logistic regression][1]. This is a very simple, linear model. In the exercises you can look at optimizing it a bit more.\n",
    "* logistic function: $f(x) = \\frac{1}{1+\\exp(-x)}$, $f(x): [-\\infty,\\infty] \\to [0,1]$\n",
    "* model: $y_i = f(x_i \\cdot \\beta) + \\epsilon_i$\n",
    "\n",
    "[1]: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver = \"lbfgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lbfgs = limited-memory BFGS, BFGS = Broyden–Fletcher–Goldfarb–Shanno algorithm, an iterative method for solving unconstrained nonlinear optimization problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit takes a few seconds...\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "plot_proba(df, lr, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Logistic Regression - v2\n",
    "Now repeat but with fewer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,:'DER_pt_tot'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using fewer features\n",
    "lr2 = LogisticRegression(solver = \"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2.fit(X_train.loc[:,:'DER_pt_tot'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2.score(X_test.loc[:,:'DER_pt_tot'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "plot_proba( df, lr2, X.loc[:,:'DER_pt_tot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More sophisticated model: GradientBoostingClassifier\n",
    "The [GradientBoostingClassifier][1] provides _gradient-boosted regression trees_. \n",
    "* ensemble method that combines multiple decision trees\n",
    "* \"forward stage-wise fashion: each tree tries to correct the mistakes of the previous one (steered by the `learning_rate`)\n",
    "* trees are simple (shallow), idea is to combine many \"weak learners\" \n",
    "  * each tree can only provide good predictions on part of the data, but combined they can yield a powerful model\n",
    "  \n",
    "[1]: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=50, max_depth=10,\n",
    "                                    min_samples_leaf=200,\n",
    "                                    max_features=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit takes several minutes... can look into AMS while it runs\n",
    "gbc.fit(X_train, y_train) # (and n_jobs is not supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check prob dist\n",
    "plot_proba(df, gbc, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBC also useful to judge/quantify importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for importance, key in sorted(zip(gbc.feature_importances_, X.keys())):\n",
    "    print (\"%30s %6.3f\" % (key, importance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure-of-Merit: AMS\n",
    "Let's get back to the original problem using the GradientBoostingClassifier. The Kaggle competition used the approximate median significance ([AMS][1]), as defined below, to determine how good a solution was. \n",
    "The goal is to maximize signal and minimize background, and the AMS is an approximate formula to quantify the signal significance. The maximal AMS gives best signal significance. \n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the total yield, which we will do below.\n",
    "\n",
    "[1]: AMS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median significance (AMS)\n",
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0: # not really needed\n",
    "        return 0\n",
    "    # The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob = gbc.predict_proba(X_train)[:, 1]\n",
    "y_test_prob  = gbc.predict_proba(X_test)[:, 1]\n",
    "pcut = np.percentile(y_train_prob, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pcut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now include the weights** to get proper normalization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgtsig  = df[df.Label==1].Weight\n",
    "wgtback = df[df.Label==0].Weight\n",
    "\n",
    "# the density argument makes this a normalized plot (otherwise wouldn't see the signal on linear scale)\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, bins=40, density = True)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background', weights=wgtback, **kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal', weights=wgtsig, **kwargs)\n",
    "_ = plt.legend()\n",
    "#plt.yscale('log') -- to try without density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the total weights (yields)\n",
    "sigall  = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# training-sample weights\n",
    "sigtrain  = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# test-sample weights\n",
    "sigtest  = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (Usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"All  :\", sigall, backall)\n",
    "print (\"Train:\", sigtrain, backtrain)\n",
    "print (\"Test :\", sigtest, backtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel  = weight_train.dot(np.multiply(y_train     , y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot(np.multiply(y_train == 0, y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel  = weight_test.dot(np.multiply(y_test     , y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal and background efficiency\n",
    "print (\"Train: eps_s = %f, eps_b = %f\" % (sigtrain_sel / sigtrain, backtrain_sel / backtrain))\n",
    "print (\"Test : eps_s = %f, eps_b = %f\" % (sigtest_sel / sigtest, backtest_sel / backtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to scale-up the selected yields to the (luminosity of the) original full sample\n",
    "sigtrain_sel_corr  = sigtrain_sel*sigall/sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr  = sigtest_sel*sigall/sigtest\n",
    "backtest_sel_corr = backtest_sel*backall/backtest\n",
    "\n",
    "print(\"Scaled selected yields in training sample, signal =\", sigtrain_sel_corr, \", background =\",backtrain_sel_corr)\n",
    "print(\"Scaled selected yields in test sample, signal =\", sigtest_sel_corr, \", background =\",backtest_sel_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:, 1], sample_weight = weight_test)\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "mark_threshold = pcut # mark selected threshold\n",
    "idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "plt.plot(fpr[idx], tpr[idx], 'o', markersize=10, label=\"threshold %f\" % mark_threshold, fillstyle=\"none\", mew=2)\n",
    "_ = plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TPR = true positive rate = TP / (TP+FN) = TP / P (= recall)\n",
    "* FPR = false positive rate = FP / (FP+TN) = FP / N\n",
    "\n",
    "_Note_: As we have a lot more background than signal events, we typically want to choose a point with a very low false-positive rate. While we can use the ROC curve to compare different classifiers, a better performance measure is the AMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr, backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr, backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create plot of AMS vs Pcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate AMS for different pcut\n",
    "def calc_ams( pcut ):\n",
    "    sigtest_sel  = weight_test.dot(np.multiply(y_test     , y_test_prob > pcut))\n",
    "    backtest_sel = weight_test.dot(np.multiply(y_test == 0, y_test_prob > pcut))\n",
    "    sigtest_sel_corr  = sigtest_sel*sigall/sigtest\n",
    "    backtest_sel_corr = backtest_sel*backall/backtest\n",
    "    return(ams(sigtest_sel_corr, backtest_sel_corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMS plot\n",
    "xv=np.linspace(0.,0.99,50)\n",
    "yv=np.vectorize(calc_ams)(xv)\n",
    "plt.plot(xv,yv)\n",
    "plt.xlabel('Pcut') # x-axis\n",
    "plt.ylabel('AMS')# y-axis\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "How did we do? Not too bad. Here are the scores of real submissions.\n",
    "![Comparison with submissions](data/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course a bit of a simplification from a real physics analysis, where systematics often seem to take the most time. They are ignored here.\n",
    "![Comparison with real analysis](figures/tr140415_davidRousseau_Rome_Higgs_MVA_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _systematics_: systematic uncertainties on the event yields and BDT distributions, of experimental and theoretical origin (cf. section 11 in reference analysis)\n",
    "* _categories_: the reference analysis discriminates two production mechanisms of the Higgs boson, VBF (events with two characteristic jets from vector-boson fusion) and boosted (all other events, dominated by gluon fusion)\n",
    "* _embedded_: dominant Z→τ⁺τ⁻ background is taken from \"τ-embedded Z→μ⁺μ⁻ data\"\n",
    "* _anti tau_: revert some tau-identification criterion to create an \"anti(-ID) tau\" sample (used in \"fake-factor method\" to estimate background with objects misidentified as tau leptons)\n",
    "* _control regions_: phase-space regions enriched in (one type of) background process that allow to normalize a predicted background contribution to that observed in data\n",
    "* _tt_: background process, events with pair production of top quarks ($t\\bar t$)\n",
    "* _NP_: nuisance parameters (parameters of fit model that are not of physical interest but give it more flexibility to describe the data)\n",
    "* _TMVA_: [Toolkit for Multivariate Data Analysis with ROOT][1], a ROOT-integrated project providing an ML environment for multivariate classification and regression techniques\n",
    "\n",
    "[1]: https://root.cern.ch/tmva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Your tasks\n",
    "What to work on for the rest of the day:\n",
    "1. Look at the definition of the variables in the manual to get a rough feeling for what physics they encode.\n",
    "1. Attempt to calculate the AMS for the logistic regression cases.\n",
    "1. Do we overfit? Add plots to see.\n",
    "1. Which variables are important?\n",
    "1. Should we **[preprocess](http://scikit-learn.org/stable/modules/preprocessing.html)** the input data to be the same scale? Note that we have some -999 values that indicate the variable could not be calculated.\n",
    "1. We do not use the event weights in the training. Can they help? Note, that you don't want to just apply the weights as is since they will make background dominate over signal.\n",
    "1. The best scores in the Challenge all used cross-validation; if you have time, try to implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Later we will continue on this example with neural networks.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
